{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate cancer subtype classification model with main classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from compath_revolutions.constants import *\n",
    "from pathway_forte.multiclass_prediction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANCER_SUBTYPES = os.path.join(DATA,'tcga_datasets','brca','brca_subtypes_matrix.txt')\n",
    "brca_subtypes_df = pd.read_csv(CANCER_SUBTYPES, sep='\\t')\n",
    "\n",
    "kegg_ssgsea_path = os.path.join(KEGG_SSGSEA, 'kegg_brca.tsv')\n",
    "wikipathways_ssgsea_path = os.path.join(WIKIPATHWAYS_SSGSEA, 'wikipathways_brca.tsv')\n",
    "reactome_ssgsea_path = os.path.join(REACTOME_SSGSEA, 'reactome_brca.tsv')\n",
    "merge_ssgsea_path = os.path.join(MERGE_SSGSEA, 'merge_brca.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample IDs and corresponding cancer subtypes\n",
    "patient_ids = get_sample_ids_with_cancer_subtypes(CANCER_SUBTYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ssGSEA scores dataFrame \n",
    "kegg_enrichment_score_df = stabilize_ssgsea_scores_df(kegg_ssgsea_path) \n",
    "wikipathways_enrichment_score_df = stabilize_ssgsea_scores_df(wikipathways_ssgsea_path) \n",
    "reactome_enrichment_score_df = stabilize_ssgsea_scores_df(reactome_ssgsea_path)\n",
    "merge_enrichment_score_df = stabilize_ssgsea_scores_df(merge_ssgsea_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match sample IDs in ssGSEA scores dataFrame with those in cancer subtype list such that only cancer patients with specified cancer subtypes are retained in the scores dataFrame. This filters out all control samples and any cancer cases with normal or NA type cancer subtype. A total of 1050 are retained from the complete set of 1215 samples. TCGA reports 5 cancer subtypes: normal, basal, Her2, LumA and LumB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kegg_pathway_features = match_samples(kegg_enrichment_score_df,patient_ids)\n",
    "reactome_pathway_features = match_samples(reactome_enrichment_score_df,patient_ids)\n",
    "wikipathways_pathway_features = match_samples(wikipathways_enrichment_score_df,patient_ids)\n",
    "merged_pathway_features = match_samples(merge_enrichment_score_df,patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples by features/pathways for each resource are:\n",
      "KEGG: (1050, 311)\n",
      "Reactome: (1050, 1170)\n",
      "WikiPathways: (1050, 362)\n",
      "PathwayForte: (1050, 1726)\n"
     ]
    }
   ],
   "source": [
    "print('The number of samples by features/pathways for each resource are:')\n",
    "print('KEGG: {}'.format(kegg_pathway_features.shape))\n",
    "print('Reactome: {}'.format(reactome_pathway_features.shape))\n",
    "print('WikiPathways: {}'.format(wikipathways_pathway_features.shape))\n",
    "print('PathwayForte: {}'.format(merged_pathway_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get arrays of class labels ordered the same way as features\n",
    "kegg_class_labels = get_class_labels(kegg_pathway_features, brca_subtypes_df)\n",
    "reactome_class_labels = get_class_labels(reactome_pathway_features, brca_subtypes_df)\n",
    "wikipathways_class_labels = get_class_labels(wikipathways_pathway_features, brca_subtypes_df)\n",
    "merged_class_labels = get_class_labels(merged_pathway_features, brca_subtypes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "1it [01:16, 76.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 1:\n",
      "test accuracy is 0.8238095238095238\n",
      "F1 score is 0.8679245283018868\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.89      0.94      0.91       114\n",
      "     Class 1       0.78      0.70      0.74        44\n",
      "     Class 2       1.00      0.67      0.80        12\n",
      "     Class 3       0.97      0.95      0.96        40\n",
      "\n",
      "   micro avg       0.89      0.88      0.88       210\n",
      "   macro avg       0.91      0.81      0.85       210\n",
      "weighted avg       0.89      0.88      0.88       210\n",
      " samples avg       0.85      0.88      0.86       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "2it [02:14, 67.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 2:\n",
      "test accuracy is 0.8095238095238095\n",
      "F1 score is 0.8634920634920635\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.92      0.92       121\n",
      "     Class 1       0.68      0.68      0.68        37\n",
      "     Class 2       0.65      0.73      0.69        15\n",
      "     Class 3       1.00      0.92      0.96        37\n",
      "\n",
      "   micro avg       0.87      0.86      0.87       210\n",
      "   macro avg       0.81      0.81      0.81       210\n",
      "weighted avg       0.87      0.86      0.87       210\n",
      " samples avg       0.84      0.86      0.84       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "3it [03:09, 63.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 3:\n",
      "test accuracy is 0.7761904761904762\n",
      "F1 score is 0.8317460317460317\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.90      0.93      0.91       112\n",
      "     Class 1       0.66      0.59      0.62        46\n",
      "     Class 2       0.69      0.53      0.60        17\n",
      "     Class 3       0.97      1.00      0.99        35\n",
      "\n",
      "   micro avg       0.85      0.83      0.84       210\n",
      "   macro avg       0.80      0.76      0.78       210\n",
      "weighted avg       0.84      0.83      0.84       210\n",
      " samples avg       0.80      0.83      0.81       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "4it [04:05, 61.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 4:\n",
      "test accuracy is 0.8095238095238095\n",
      "F1 score is 0.8466666666666666\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.94      0.93       114\n",
      "     Class 1       0.71      0.49      0.58        41\n",
      "     Class 2       0.88      0.74      0.80        19\n",
      "     Class 3       1.00      1.00      1.00        36\n",
      "\n",
      "   micro avg       0.90      0.84      0.87       210\n",
      "   macro avg       0.88      0.79      0.83       210\n",
      "weighted avg       0.89      0.84      0.86       210\n",
      " samples avg       0.83      0.84      0.83       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "5it [04:57, 59.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 5:\n",
      "test accuracy is 0.7904761904761904\n",
      "F1 score is 0.8421052631578947\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.89      0.90      0.89       106\n",
      "     Class 1       0.76      0.64      0.69        39\n",
      "     Class 2       0.69      0.58      0.63        19\n",
      "     Class 3       0.98      0.98      0.98        46\n",
      "\n",
      "   micro avg       0.87      0.84      0.85       210\n",
      "   macro avg       0.83      0.77      0.80       210\n",
      "weighted avg       0.87      0.84      0.85       210\n",
      " samples avg       0.81      0.84      0.82       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kegg_all_metrics = train_multiclass_log_reg(\n",
    "                        kegg_pathway_features, \n",
    "                        kegg_class_labels, \n",
    "                        inner_cv=5, \n",
    "                        outer_cv=5,\n",
    "                        chain_pca=False, \n",
    "                        explained_variance=0.95    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "1it [03:15, 195.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 1:\n",
      "test accuracy is 0.8428571428571429\n",
      "F1 score is 0.8933333333333334\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.97      0.91      0.94       123\n",
      "     Class 1       0.76      0.67      0.71        33\n",
      "     Class 2       0.77      0.77      0.77        13\n",
      "     Class 3       0.98      0.98      0.98        41\n",
      "\n",
      "   micro avg       0.93      0.88      0.90       210\n",
      "   macro avg       0.87      0.83      0.85       210\n",
      "weighted avg       0.93      0.88      0.90       210\n",
      " samples avg       0.86      0.88      0.87       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "2it [06:24, 192.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 2:\n",
      "test accuracy is 0.819047619047619\n",
      "F1 score is 0.8606060606060606\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.92      0.92       118\n",
      "     Class 1       0.67      0.77      0.72        43\n",
      "     Class 2       0.86      0.63      0.73        19\n",
      "     Class 3       1.00      1.00      1.00        30\n",
      "\n",
      "   micro avg       0.86      0.88      0.87       210\n",
      "   macro avg       0.86      0.83      0.84       210\n",
      "weighted avg       0.87      0.88      0.87       210\n",
      " samples avg       0.85      0.88      0.86       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "3it [09:40, 193.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 3:\n",
      "test accuracy is 0.8714285714285714\n",
      "F1 score is 0.9114754098360655\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.95      0.94       109\n",
      "     Class 1       0.88      0.80      0.83        44\n",
      "     Class 2       0.85      0.69      0.76        16\n",
      "     Class 3       1.00      1.00      1.00        41\n",
      "\n",
      "   micro avg       0.93      0.91      0.92       210\n",
      "   macro avg       0.91      0.86      0.88       210\n",
      "weighted avg       0.93      0.91      0.92       210\n",
      " samples avg       0.89      0.91      0.90       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "4it [13:02, 195.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 4:\n",
      "test accuracy is 0.8571428571428571\n",
      "F1 score is 0.9078947368421052\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.96      0.95       112\n",
      "     Class 1       0.89      0.72      0.79        43\n",
      "     Class 2       0.81      0.68      0.74        19\n",
      "     Class 3       0.95      0.97      0.96        36\n",
      "\n",
      "   micro avg       0.92      0.89      0.90       210\n",
      "   macro avg       0.90      0.83      0.86       210\n",
      "weighted avg       0.92      0.89      0.90       210\n",
      " samples avg       0.87      0.89      0.88       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "5it [16:22, 196.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 5:\n",
      "test accuracy is 0.8571428571428571\n",
      "F1 score is 0.8927335640138407\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.93      0.92       105\n",
      "     Class 1       0.97      0.70      0.82        44\n",
      "     Class 2       0.93      0.87      0.90        15\n",
      "     Class 3       1.00      0.96      0.98        46\n",
      "\n",
      "   micro avg       0.94      0.89      0.91       210\n",
      "   macro avg       0.95      0.87      0.90       210\n",
      "weighted avg       0.94      0.89      0.91       210\n",
      " samples avg       0.87      0.89      0.88       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reactome_all_metrics = train_multiclass_log_reg(\n",
    "                        reactome_pathway_features, \n",
    "                        reactome_class_labels,\n",
    "                        inner_cv=5, \n",
    "                        outer_cv=5,\n",
    "                        chain_pca=False, \n",
    "                        explained_variance=0.95    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "1it [01:12, 72.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 1:\n",
      "test accuracy is 0.8428571428571429\n",
      "F1 score is 0.872852233676976\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.90      0.97      0.93        98\n",
      "     Class 1       0.86      0.64      0.74        50\n",
      "     Class 2       1.00      0.78      0.88        18\n",
      "     Class 3       0.98      0.98      0.98        44\n",
      "\n",
      "   micro avg       0.92      0.88      0.90       210\n",
      "   macro avg       0.93      0.84      0.88       210\n",
      "weighted avg       0.91      0.88      0.89       210\n",
      " samples avg       0.86      0.88      0.87       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "2it [02:06, 63.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 2:\n",
      "test accuracy is 0.8333333333333334\n",
      "F1 score is 0.8634920634920635\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.94      0.93       119\n",
      "     Class 1       0.69      0.60      0.64        40\n",
      "     Class 2       0.87      0.72      0.79        18\n",
      "     Class 3       1.00      0.97      0.98        33\n",
      "\n",
      "   micro avg       0.89      0.86      0.88       210\n",
      "   macro avg       0.87      0.81      0.84       210\n",
      "weighted avg       0.89      0.86      0.87       210\n",
      " samples avg       0.85      0.86      0.85       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "3it [03:00, 60.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 3:\n",
      "test accuracy is 0.8714285714285714\n",
      "F1 score is 0.9065743944636678\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.97      0.95       111\n",
      "     Class 1       0.74      0.74      0.74        31\n",
      "     Class 2       0.86      0.67      0.75        18\n",
      "     Class 3       1.00      1.00      1.00        50\n",
      "\n",
      "   micro avg       0.91      0.92      0.92       210\n",
      "   macro avg       0.88      0.85      0.86       210\n",
      "weighted avg       0.91      0.92      0.91       210\n",
      " samples avg       0.90      0.92      0.90       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "4it [03:50, 57.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 4:\n",
      "test accuracy is 0.8571428571428571\n",
      "F1 score is 0.8982035928143712\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.96      0.91      0.94       129\n",
      "     Class 1       0.76      0.80      0.78        40\n",
      "     Class 2       0.71      0.92      0.80        13\n",
      "     Class 3       1.00      0.96      0.98        28\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       210\n",
      "   macro avg       0.86      0.90      0.87       210\n",
      "weighted avg       0.91      0.90      0.90       210\n",
      " samples avg       0.88      0.90      0.89       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "5it [04:41, 56.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 5:\n",
      "test accuracy is 0.819047619047619\n",
      "F1 score is 0.8799999999999999\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.94      0.94       110\n",
      "     Class 1       0.81      0.63      0.71        46\n",
      "     Class 2       0.80      0.53      0.64        15\n",
      "     Class 3       0.97      0.95      0.96        39\n",
      "\n",
      "   micro avg       0.92      0.84      0.88       210\n",
      "   macro avg       0.88      0.76      0.81       210\n",
      "weighted avg       0.91      0.84      0.87       210\n",
      " samples avg       0.83      0.84      0.83       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wikipathways_all_metrics = train_multiclass_log_reg(\n",
    "                            wikipathways_pathway_features, \n",
    "                            wikipathways_class_labels,\n",
    "                            inner_cv=5, \n",
    "                            outer_cv=5,\n",
    "                            chain_pca=False, \n",
    "                            explained_variance=0.95    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "1it [04:03, 243.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 1:\n",
      "test accuracy is 0.8571428571428571\n",
      "F1 score is 0.904109589041096\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.95      0.94       106\n",
      "     Class 1       0.91      0.72      0.81        43\n",
      "     Class 2       0.85      0.65      0.73        17\n",
      "     Class 3       1.00      0.98      0.99        44\n",
      "\n",
      "   micro avg       0.93      0.89      0.91       210\n",
      "   macro avg       0.92      0.82      0.87       210\n",
      "weighted avg       0.93      0.89      0.91       210\n",
      " samples avg       0.87      0.89      0.88       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "2it [07:37, 228.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 2:\n",
      "test accuracy is 0.8047619047619048\n",
      "F1 score is 0.8627450980392156\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.90      0.94      0.92       113\n",
      "     Class 1       0.81      0.60      0.69        43\n",
      "     Class 2       0.60      0.60      0.60        15\n",
      "     Class 3       1.00      0.95      0.97        39\n",
      "\n",
      "   micro avg       0.88      0.85      0.86       210\n",
      "   macro avg       0.83      0.77      0.80       210\n",
      "weighted avg       0.88      0.85      0.86       210\n",
      " samples avg       0.83      0.85      0.83       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "3it [11:54, 238.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 3:\n",
      "test accuracy is 0.8571428571428571\n",
      "F1 score is 0.8761904761904762\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.91      0.92       109\n",
      "     Class 1       0.75      0.83      0.79        47\n",
      "     Class 2       1.00      0.88      0.93        16\n",
      "     Class 3       1.00      0.97      0.99        38\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       210\n",
      "   macro avg       0.92      0.90      0.91       210\n",
      "weighted avg       0.91      0.90      0.90       210\n",
      " samples avg       0.88      0.90      0.89       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "4it [16:02, 240.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 4:\n",
      "test accuracy is 0.9047619047619048\n",
      "F1 score is 0.9430379746835443\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      0.96      0.97       125\n",
      "     Class 1       0.78      0.91      0.84        32\n",
      "     Class 2       1.00      0.67      0.80        15\n",
      "     Class 3       0.95      1.00      0.97        38\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       210\n",
      "   macro avg       0.93      0.88      0.90       210\n",
      "weighted avg       0.95      0.94      0.94       210\n",
      " samples avg       0.92      0.94      0.93       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\r",
      "5it [20:11, 242.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 5:\n",
      "test accuracy is 0.8523809523809524\n",
      "F1 score is 0.8888888888888887\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.96      0.94       114\n",
      "     Class 1       0.82      0.64      0.72        42\n",
      "     Class 2       0.83      0.79      0.81        19\n",
      "     Class 3       1.00      1.00      1.00        35\n",
      "\n",
      "   micro avg       0.92      0.89      0.90       210\n",
      "   macro avg       0.90      0.85      0.87       210\n",
      "weighted avg       0.91      0.89      0.90       210\n",
      " samples avg       0.87      0.89      0.87       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "merged_all_metrics = train_multiclass_log_reg(\n",
    "                        merged_pathway_features, \n",
    "                        merged_class_labels,\n",
    "                        inner_cv=5, \n",
    "                        outer_cv=5,\n",
    "                        chain_pca=False, \n",
    "                        explained_variance=0.95    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [{'Accuracy': 0.8238095238095238,\n",
       "               'F1 score': 0.8679245283018868,\n",
       "               'Precision': 0.8888888888888888,\n",
       "               'Recall': 0.8761904761904762}],\n",
       "             2: [{'Accuracy': 0.8095238095238095,\n",
       "               'F1 score': 0.8634920634920635,\n",
       "               'Precision': 0.8701923076923077,\n",
       "               'Recall': 0.861904761904762}],\n",
       "             3: [{'Accuracy': 0.7761904761904762,\n",
       "               'F1 score': 0.8317460317460317,\n",
       "               'Precision': 0.8495145631067961,\n",
       "               'Recall': 0.8333333333333334}],\n",
       "             4: [{'Accuracy': 0.8095238095238095,\n",
       "               'F1 score': 0.8466666666666666,\n",
       "               'Precision': 0.8984771573604061,\n",
       "               'Recall': 0.8428571428571429}],\n",
       "             5: [{'Accuracy': 0.7904761904761904,\n",
       "               'F1 score': 0.8421052631578947,\n",
       "               'Precision': 0.8712871287128713,\n",
       "               'Recall': 0.8380952380952381}]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kegg_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [{'Accuracy': 0.8428571428571429,\n",
       "               'F1 score': 0.8933333333333334,\n",
       "               'Precision': 0.9292929292929293,\n",
       "               'Recall': 0.8761904761904762}],\n",
       "             2: [{'Accuracy': 0.819047619047619,\n",
       "               'F1 score': 0.8606060606060606,\n",
       "               'Precision': 0.863849765258216,\n",
       "               'Recall': 0.8761904761904762}],\n",
       "             3: [{'Accuracy': 0.8714285714285714,\n",
       "               'F1 score': 0.9114754098360655,\n",
       "               'Precision': 0.9271844660194175,\n",
       "               'Recall': 0.9095238095238095}],\n",
       "             4: [{'Accuracy': 0.8571428571428571,\n",
       "               'F1 score': 0.9078947368421052,\n",
       "               'Precision': 0.9207920792079208,\n",
       "               'Recall': 0.8857142857142857}],\n",
       "             5: [{'Accuracy': 0.8571428571428571,\n",
       "               'F1 score': 0.8927335640138407,\n",
       "               'Precision': 0.9393939393939394,\n",
       "               'Recall': 0.8857142857142857}]})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reactome_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [{'Accuracy': 0.8428571428571429,\n",
       "               'F1 score': 0.872852233676976,\n",
       "               'Precision': 0.9154228855721394,\n",
       "               'Recall': 0.8761904761904762}],\n",
       "             2: [{'Accuracy': 0.8333333333333334,\n",
       "               'F1 score': 0.8634920634920635,\n",
       "               'Precision': 0.8916256157635468,\n",
       "               'Recall': 0.861904761904762}],\n",
       "             3: [{'Accuracy': 0.8714285714285714,\n",
       "               'F1 score': 0.9065743944636678,\n",
       "               'Precision': 0.9146919431279621,\n",
       "               'Recall': 0.919047619047619}],\n",
       "             4: [{'Accuracy': 0.8571428571428571,\n",
       "               'F1 score': 0.8982035928143712,\n",
       "               'Precision': 0.9043062200956937,\n",
       "               'Recall': 0.9}],\n",
       "             5: [{'Accuracy': 0.819047619047619,\n",
       "               'F1 score': 0.8799999999999999,\n",
       "               'Precision': 0.921875,\n",
       "               'Recall': 0.8428571428571429}]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipathways_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [{'Accuracy': 0.8571428571428571,\n",
       "               'F1 score': 0.904109589041096,\n",
       "               'Precision': 0.9346733668341709,\n",
       "               'Recall': 0.8857142857142857}],\n",
       "             2: [{'Accuracy': 0.8047619047619048,\n",
       "               'F1 score': 0.8627450980392156,\n",
       "               'Precision': 0.8811881188118812,\n",
       "               'Recall': 0.8476190476190476}],\n",
       "             3: [{'Accuracy': 0.8571428571428571,\n",
       "               'F1 score': 0.8761904761904762,\n",
       "               'Precision': 0.9,\n",
       "               'Recall': 0.9}],\n",
       "             4: [{'Accuracy': 0.9047619047619048,\n",
       "               'F1 score': 0.9430379746835443,\n",
       "               'Precision': 0.9425837320574163,\n",
       "               'Recall': 0.9380952380952381}],\n",
       "             5: [{'Accuracy': 0.8523809523809524,\n",
       "               'F1 score': 0.8888888888888887,\n",
       "               'Precision': 0.916256157635468,\n",
       "               'Recall': 0.8857142857142857}]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
