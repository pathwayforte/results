{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate cancer subtype classification SVM model with main classification metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathway_forte.constants import *\n",
    "from pathway_forte.multiclass_prediction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CANCER_SUBTYPES = os.path.join(DATA,'tcga_datasets','brca','brca_subtypes_matrix.txt')\n",
    "brca_subtypes_df = pd.read_csv(CANCER_SUBTYPES, sep='\\t')\n",
    "\n",
    "kegg_ssgsea_path = os.path.join(KEGG_SSGSEA, 'kegg_brca.tsv')\n",
    "wikipathways_ssgsea_path = os.path.join(WIKIPATHWAYS_SSGSEA, 'wikipathways_brca.tsv')\n",
    "reactome_ssgsea_path = os.path.join(REACTOME_SSGSEA, 'reactome_brca.tsv')\n",
    "merge_ssgsea_path = os.path.join(MERGE_SSGSEA, 'merge_brca.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get sample IDs and corresponding cancer subtypes\n",
    "patient_ids = get_sample_ids_with_cancer_subtypes(CANCER_SUBTYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get ssGSEA scores dataFrame \n",
    "kegg_enrichment_score_df = stabilize_ssgsea_scores_df(kegg_ssgsea_path) \n",
    "wikipathways_enrichment_score_df = stabilize_ssgsea_scores_df(wikipathways_ssgsea_path) \n",
    "reactome_enrichment_score_df = stabilize_ssgsea_scores_df(reactome_ssgsea_path)\n",
    "merge_enrichment_score_df = stabilize_ssgsea_scores_df(merge_ssgsea_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match sample IDs in ssGSEA scores dataFrame with those in cancer subtype list such that only cancer patients with specified cancer subtypes are retained in the scores dataFrame. This filters out all control samples and any cancer cases with normal or NA type cancer subtype. A total of 1050 are retained from the complete set of 1215 samples. TCGA reports 5 cancer subtypes: normal, basal, Her2, LumA and LumB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kegg_pathway_features = match_samples(kegg_enrichment_score_df,patient_ids)\n",
    "reactome_pathway_features = match_samples(reactome_enrichment_score_df,patient_ids)\n",
    "wikipathways_pathway_features = match_samples(wikipathways_enrichment_score_df,patient_ids)\n",
    "merged_pathway_features = match_samples(merge_enrichment_score_df,patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples by features/pathways for each resource are:\n",
      "KEGG: (1050, 311)\n",
      "Reactome: (1050, 1170)\n",
      "WikiPathways: (1050, 362)\n",
      "PathwayForte: (1050, 1726)\n"
     ]
    }
   ],
   "source": [
    "print('The number of samples by features/pathways for each resource are:')\n",
    "print('KEGG: {}'.format(kegg_pathway_features.shape))\n",
    "print('Reactome: {}'.format(reactome_pathway_features.shape))\n",
    "print('WikiPathways: {}'.format(wikipathways_pathway_features.shape))\n",
    "print('PathwayForte: {}'.format(merged_pathway_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get arrays of class labels ordered the same way as features\n",
    "kegg_class_labels = get_class_labels(kegg_pathway_features, brca_subtypes_df)\n",
    "reactome_class_labels = get_class_labels(reactome_pathway_features, brca_subtypes_df)\n",
    "wikipathways_class_labels = get_class_labels(wikipathways_pathway_features, brca_subtypes_df)\n",
    "merged_class_labels = get_class_labels(merged_pathway_features, brca_subtypes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_df_to_features_array(df):\n",
    "    \n",
    "    # Get list of pathways as features\n",
    "    feature_cols = list(df.columns.values)\n",
    "\n",
    "    # Features\n",
    "    pathways = df[feature_cols]  # Features\n",
    "\n",
    "    # Transform features dataFrame to numpy array\n",
    "    pathways_array = pathways.values\n",
    "\n",
    "    return np.asarray(pathways_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kegg_features_array = convert_df_to_features_array(kegg_pathway_features)\n",
    "reactome_features_array = convert_df_to_features_array(reactome_pathway_features)\n",
    "wikipathways_features_array = convert_df_to_features_array(wikipathways_pathway_features)\n",
    "merged_features_array = convert_df_to_features_array(merged_pathway_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_multiclass_log_reg(X, y, inner_cv, outer_cv, chain_pca=False, explained_variance=0.95, roc_auc=False):\n",
    "\n",
    "    all_metrics = defaultdict(list)\n",
    "        \n",
    "  #  y = label_binarize(y, classes=[0,1,2,3])\n",
    "\n",
    "    target_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3']\n",
    "    \n",
    "    kf = KFold(n_splits=outer_cv, shuffle=True)\n",
    "\n",
    "    iterator = tqdm(kf.split(X, y))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(iterator):\n",
    "\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train, y_test = np.asarray([y[i] for i in train_index]), np.asarray(\n",
    "        [y[i] for i in test_index])\n",
    "        \n",
    "        if chain_pca:\n",
    "            # Apply PCA\n",
    "            X_train, X_test = pca_chaining(X_train, X_test, explained_variance)\n",
    "\n",
    "        # Fit one classifier per class \n",
    "        # For each classifier, class is fit against all other classes\n",
    "        classifier = OneVsOneClassifier(\n",
    "                        LinearSVC()\n",
    "        )\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        # Get the subset accuracy st labels predicted for a sample exactly match true labels (harsh)\n",
    "        accurcay = metrics.accuracy_score(y_test, y_pred) # set sample_weight to get weighted accuracy\n",
    "        f1_score = metrics.f1_score(y_test, y_pred, average='micro', labels=np.unique(y_pred))\n",
    "        \n",
    "        metrics_dict = {\n",
    "            'Accuracy': accurcay,\n",
    "            'F1 score': f1_score,\n",
    "            'Precision': metrics.precision_score(y_test, y_pred, average='micro'),\n",
    "            'Recall': metrics.recall_score(y_test, y_pred, average='micro')\n",
    "        }\n",
    "        \n",
    "        all_metrics[i+1].append(metrics_dict)\n",
    "       \n",
    "        print('For iteration {}:'.format(i+1))\n",
    "        print('test accuracy is {}'.format(accurcay))\n",
    "        print('F1 score is {}'.format(f1_score))\n",
    "        print(\"\\n\")\n",
    "        print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "        \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1it [00:00,  2.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 1:\n",
      "test accuracy is 0.8666666666666667\n",
      "F1 score is 0.8666666666666667\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.87      0.96      0.91       117\n",
      "     Class 1       0.77      0.61      0.68        38\n",
      "     Class 2       0.77      0.59      0.67        17\n",
      "     Class 3       0.97      0.97      0.97        38\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       210\n",
      "   macro avg       0.84      0.78      0.81       210\n",
      "weighted avg       0.86      0.87      0.86       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2it [00:00,  2.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 2:\n",
      "test accuracy is 0.9047619047619048\n",
      "F1 score is 0.9047619047619048\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.88      0.97      0.93       109\n",
      "     Class 1       0.90      0.70      0.79        40\n",
      "     Class 2       0.83      0.71      0.77        14\n",
      "     Class 3       0.98      0.98      0.98        47\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       210\n",
      "   macro avg       0.90      0.84      0.87       210\n",
      "weighted avg       0.91      0.90      0.90       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3it [00:01,  2.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 3:\n",
      "test accuracy is 0.8809523809523809\n",
      "F1 score is 0.8809523809523809\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.87      0.95      0.91       105\n",
      "     Class 1       0.80      0.70      0.75        47\n",
      "     Class 2       0.92      0.73      0.81        15\n",
      "     Class 3       0.98      0.95      0.96        43\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       210\n",
      "   macro avg       0.89      0.84      0.86       210\n",
      "weighted avg       0.88      0.88      0.88       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4it [00:01,  2.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 4:\n",
      "test accuracy is 0.8952380952380953\n",
      "F1 score is 0.8952380952380953\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.94      0.93       118\n",
      "     Class 1       0.73      0.77      0.75        39\n",
      "     Class 2       1.00      0.67      0.80        18\n",
      "     Class 3       1.00      1.00      1.00        35\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       210\n",
      "   macro avg       0.91      0.84      0.87       210\n",
      "weighted avg       0.90      0.90      0.89       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5it [00:01,  2.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 5:\n",
      "test accuracy is 0.8095238095238095\n",
      "F1 score is 0.8095238095238095\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.86      0.90      0.88       118\n",
      "     Class 1       0.62      0.58      0.60        43\n",
      "     Class 2       0.64      0.50      0.56        18\n",
      "     Class 3       0.91      0.97      0.94        31\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       210\n",
      "   macro avg       0.76      0.74      0.75       210\n",
      "weighted avg       0.80      0.81      0.80       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kegg_all_metrics = train_multiclass_log_reg(\n",
    "                        kegg_features_array, \n",
    "                        kegg_class_labels, \n",
    "                        inner_cv=5, \n",
    "                        outer_cv=5,\n",
    "                        chain_pca=True, \n",
    "                        explained_variance=0.95    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1it [00:02,  2.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 1:\n",
      "test accuracy is 0.9\n",
      "F1 score is 0.9\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.92      0.93      0.92       109\n",
      "     Class 1       0.77      0.77      0.77        39\n",
      "     Class 2       0.83      0.71      0.77        14\n",
      "     Class 3       0.98      1.00      0.99        48\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       210\n",
      "   macro avg       0.88      0.85      0.86       210\n",
      "weighted avg       0.90      0.90      0.90       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2it [00:06,  3.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 2:\n",
      "test accuracy is 0.919047619047619\n",
      "F1 score is 0.919047619047619\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.94      0.94       112\n",
      "     Class 1       0.80      0.89      0.84        44\n",
      "     Class 2       0.92      0.75      0.83        16\n",
      "     Class 3       1.00      0.97      0.99        38\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       210\n",
      "   macro avg       0.92      0.89      0.90       210\n",
      "weighted avg       0.92      0.92      0.92       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3it [00:10,  3.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 3:\n",
      "test accuracy is 0.9142857142857143\n",
      "F1 score is 0.9142857142857143\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.93      0.93       120\n",
      "     Class 1       0.82      0.82      0.82        44\n",
      "     Class 2       0.87      1.00      0.93        13\n",
      "     Class 3       1.00      0.97      0.98        33\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       210\n",
      "   macro avg       0.90      0.93      0.92       210\n",
      "weighted avg       0.92      0.91      0.91       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4it [00:13,  3.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 4:\n",
      "test accuracy is 0.9238095238095239\n",
      "F1 score is 0.9238095238095239\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.98      0.96       110\n",
      "     Class 1       0.88      0.77      0.82        39\n",
      "     Class 2       0.90      0.86      0.88        21\n",
      "     Class 3       0.95      0.95      0.95        40\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       210\n",
      "   macro avg       0.92      0.89      0.90       210\n",
      "weighted avg       0.92      0.92      0.92       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5it [00:16,  3.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 5:\n",
      "test accuracy is 0.9095238095238095\n",
      "F1 score is 0.9095238095238095\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.93      0.94       116\n",
      "     Class 1       0.79      0.83      0.81        41\n",
      "     Class 2       0.78      0.78      0.78        18\n",
      "     Class 3       1.00      1.00      1.00        35\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       210\n",
      "   macro avg       0.88      0.88      0.88       210\n",
      "weighted avg       0.91      0.91      0.91       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reactome_all_metrics = train_multiclass_log_reg(\n",
    "                        r_features_array, \n",
    "                        reactome_class_labels,\n",
    "                        inner_cv=5, \n",
    "                        outer_cv=5,\n",
    "                        chain_pca=True, \n",
    "                        explained_variance=0.95    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1it [00:00,  2.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 1:\n",
      "test accuracy is 0.9\n",
      "F1 score is 0.9\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.95      0.94       106\n",
      "     Class 1       0.76      0.85      0.80        46\n",
      "     Class 2       1.00      0.47      0.64        17\n",
      "     Class 3       0.95      1.00      0.98        41\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       210\n",
      "   macro avg       0.91      0.82      0.84       210\n",
      "weighted avg       0.91      0.90      0.89       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2it [00:00,  2.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 2:\n",
      "test accuracy is 0.9238095238095239\n",
      "F1 score is 0.9238095238095239\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.96      0.96       111\n",
      "     Class 1       0.86      0.82      0.84        44\n",
      "     Class 2       0.71      0.77      0.74        13\n",
      "     Class 3       1.00      0.98      0.99        42\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       210\n",
      "   macro avg       0.88      0.88      0.88       210\n",
      "weighted avg       0.92      0.92      0.92       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3it [00:01,  2.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 3:\n",
      "test accuracy is 0.9238095238095239\n",
      "F1 score is 0.9238095238095239\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.96      0.95       122\n",
      "     Class 1       0.76      0.79      0.77        28\n",
      "     Class 2       0.85      0.73      0.79        15\n",
      "     Class 3       1.00      0.98      0.99        45\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       210\n",
      "   macro avg       0.89      0.86      0.87       210\n",
      "weighted avg       0.92      0.92      0.92       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4it [00:01,  2.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 4:\n",
      "test accuracy is 0.9047619047619048\n",
      "F1 score is 0.9047619047619048\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.95      0.94       118\n",
      "     Class 1       0.71      0.77      0.74        35\n",
      "     Class 2       1.00      0.75      0.86        20\n",
      "     Class 3       1.00      0.97      0.99        37\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       210\n",
      "   macro avg       0.91      0.86      0.88       210\n",
      "weighted avg       0.91      0.90      0.91       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5it [00:02,  2.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 5:\n",
      "test accuracy is 0.8904761904761904\n",
      "F1 score is 0.8904761904761904\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.87      0.97      0.92       110\n",
      "     Class 1       0.90      0.69      0.78        54\n",
      "     Class 2       0.88      0.82      0.85        17\n",
      "     Class 3       0.97      1.00      0.98        29\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       210\n",
      "   macro avg       0.90      0.87      0.88       210\n",
      "weighted avg       0.89      0.89      0.89       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikipathways_all_metrics = train_multiclass_log_reg(\n",
    "                            wikipathways_features_array, \n",
    "                            wikipathways_class_labels,\n",
    "                            inner_cv=5, \n",
    "                            outer_cv=5,\n",
    "                            chain_pca=True, \n",
    "                            explained_variance=0.95    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1it [00:04,  4.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 1:\n",
      "test accuracy is 0.919047619047619\n",
      "F1 score is 0.919047619047619\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.92      0.98      0.95       100\n",
      "     Class 1       0.89      0.82      0.85        50\n",
      "     Class 2       0.76      0.72      0.74        18\n",
      "     Class 3       1.00      0.98      0.99        42\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       210\n",
      "   macro avg       0.90      0.87      0.88       210\n",
      "weighted avg       0.92      0.92      0.92       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2it [00:09,  4.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 2:\n",
      "test accuracy is 0.9095238095238095\n",
      "F1 score is 0.9095238095238095\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.92      0.93       125\n",
      "     Class 1       0.71      0.81      0.76        31\n",
      "     Class 2       0.89      0.84      0.86        19\n",
      "     Class 3       0.97      1.00      0.99        35\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       210\n",
      "   macro avg       0.88      0.89      0.89       210\n",
      "weighted avg       0.91      0.91      0.91       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3it [00:14,  4.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 3:\n",
      "test accuracy is 0.9\n",
      "F1 score is 0.9\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.94      0.93       108\n",
      "     Class 1       0.85      0.74      0.80        47\n",
      "     Class 2       0.76      0.87      0.81        15\n",
      "     Class 3       0.97      0.97      0.97        40\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       210\n",
      "   macro avg       0.88      0.88      0.88       210\n",
      "weighted avg       0.90      0.90      0.90       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4it [00:20,  5.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 4:\n",
      "test accuracy is 0.9333333333333333\n",
      "F1 score is 0.9333333333333333\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.94      0.95      0.94       117\n",
      "     Class 1       0.83      0.83      0.83        35\n",
      "     Class 2       1.00      0.88      0.94        17\n",
      "     Class 3       0.98      1.00      0.99        41\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       210\n",
      "   macro avg       0.94      0.91      0.92       210\n",
      "weighted avg       0.93      0.93      0.93       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/sarah/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5it [00:25,  5.09s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 5:\n",
      "test accuracy is 0.9238095238095239\n",
      "F1 score is 0.9238095238095239\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.94      0.94       117\n",
      "     Class 1       0.81      0.89      0.85        44\n",
      "     Class 2       1.00      0.77      0.87        13\n",
      "     Class 3       0.97      0.97      0.97        36\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       210\n",
      "   macro avg       0.93      0.89      0.91       210\n",
      "weighted avg       0.93      0.92      0.92       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_all_metrics = train_multiclass_log_reg(\n",
    "                        merged_features_array, \n",
    "                        merged_class_labels,\n",
    "                        inner_cv=5, \n",
    "                        outer_cv=5,\n",
    "                        chain_pca=True, \n",
    "                        explained_variance=0.95    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [{'Accuracy': 0.8666666666666667,\n",
       "               'F1 score': 0.8666666666666667,\n",
       "               'Precision': 0.8666666666666667,\n",
       "               'Recall': 0.8666666666666667}],\n",
       "             2: [{'Accuracy': 0.9047619047619048,\n",
       "               'F1 score': 0.9047619047619048,\n",
       "               'Precision': 0.9047619047619048,\n",
       "               'Recall': 0.9047619047619048}],\n",
       "             3: [{'Accuracy': 0.8809523809523809,\n",
       "               'F1 score': 0.8809523809523809,\n",
       "               'Precision': 0.8809523809523809,\n",
       "               'Recall': 0.8809523809523809}],\n",
       "             4: [{'Accuracy': 0.8952380952380953,\n",
       "               'F1 score': 0.8952380952380953,\n",
       "               'Precision': 0.8952380952380953,\n",
       "               'Recall': 0.8952380952380953}],\n",
       "             5: [{'Accuracy': 0.8095238095238095,\n",
       "               'F1 score': 0.8095238095238095,\n",
       "               'Precision': 0.8095238095238095,\n",
       "               'Recall': 0.8095238095238095}]})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kegg_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [{'Accuracy': 0.9,\n",
       "               'F1 score': 0.9,\n",
       "               'Precision': 0.9,\n",
       "               'Recall': 0.9}],\n",
       "             2: [{'Accuracy': 0.919047619047619,\n",
       "               'F1 score': 0.919047619047619,\n",
       "               'Precision': 0.919047619047619,\n",
       "               'Recall': 0.919047619047619}],\n",
       "             3: [{'Accuracy': 0.9142857142857143,\n",
       "               'F1 score': 0.9142857142857143,\n",
       "               'Precision': 0.9142857142857143,\n",
       "               'Recall': 0.9142857142857143}],\n",
       "             4: [{'Accuracy': 0.9238095238095239,\n",
       "               'F1 score': 0.9238095238095239,\n",
       "               'Precision': 0.9238095238095239,\n",
       "               'Recall': 0.9238095238095239}],\n",
       "             5: [{'Accuracy': 0.9095238095238095,\n",
       "               'F1 score': 0.9095238095238095,\n",
       "               'Precision': 0.9095238095238095,\n",
       "               'Recall': 0.9095238095238095}]})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reactome_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [{'Accuracy': 0.9,\n",
       "               'F1 score': 0.9,\n",
       "               'Precision': 0.9,\n",
       "               'Recall': 0.9}],\n",
       "             2: [{'Accuracy': 0.9238095238095239,\n",
       "               'F1 score': 0.9238095238095239,\n",
       "               'Precision': 0.9238095238095239,\n",
       "               'Recall': 0.9238095238095239}],\n",
       "             3: [{'Accuracy': 0.9238095238095239,\n",
       "               'F1 score': 0.9238095238095239,\n",
       "               'Precision': 0.9238095238095239,\n",
       "               'Recall': 0.9238095238095239}],\n",
       "             4: [{'Accuracy': 0.9047619047619048,\n",
       "               'F1 score': 0.9047619047619048,\n",
       "               'Precision': 0.9047619047619048,\n",
       "               'Recall': 0.9047619047619048}],\n",
       "             5: [{'Accuracy': 0.8904761904761904,\n",
       "               'F1 score': 0.8904761904761904,\n",
       "               'Precision': 0.8904761904761904,\n",
       "               'Recall': 0.8904761904761904}]})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipathways_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: [{'Accuracy': 0.919047619047619,\n",
       "               'F1 score': 0.919047619047619,\n",
       "               'Precision': 0.919047619047619,\n",
       "               'Recall': 0.919047619047619}],\n",
       "             2: [{'Accuracy': 0.9095238095238095,\n",
       "               'F1 score': 0.9095238095238095,\n",
       "               'Precision': 0.9095238095238095,\n",
       "               'Recall': 0.9095238095238095}],\n",
       "             3: [{'Accuracy': 0.9,\n",
       "               'F1 score': 0.9,\n",
       "               'Precision': 0.9,\n",
       "               'Recall': 0.9}],\n",
       "             4: [{'Accuracy': 0.9333333333333333,\n",
       "               'F1 score': 0.9333333333333333,\n",
       "               'Precision': 0.9333333333333333,\n",
       "               'Recall': 0.9333333333333333}],\n",
       "             5: [{'Accuracy': 0.9238095238095239,\n",
       "               'F1 score': 0.9238095238095239,\n",
       "               'Precision': 0.9238095238095239,\n",
       "               'Recall': 0.9238095238095239}]})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
